```{r}
#library(gt)
library(tidyverse)
#library(dagitty)
#library(DT)
library(rstanarm)
#library(ggdag)
library(patchwork)
```






name: teil-1
class: middle, center



# Teil 1

## Grundlagen der logistischen Regression


---


## Lineare Modelle f√ºr bin√§re AV?


>    *Forschungsfrage*: Kann man anhand des Spritverbrauchs vorhersagen, ob ein Auto eine Automatik- bzw. ein manuelle Schaltung hat? Anders gesagt: H√§ngen Spritverbrauch und Getriebeart? (Datensatz `mtcars`)


.pull-left[

```{r echo = TRUE}
data(mtcars)
mtcars <-
  mtcars %>% 
  mutate(mpg_z = scale(mpg))
```

```{r echo = TRUE, results = "show"}
m91 <-
  stan_glm(am ~ mpg_z, 
           data = mtcars, 
           refresh = 0)
coef(m91)
```


Wir k√∂nnen die Vorhersagen des Modells, d.h. $\hat{y}_i$, 
als *Wahrscheinlichkeit* interpretieren (f√ºr `am=1)`.

]

.pull-right[
```{r}
mtcars %>% 
  ggplot(aes(x = mpg_z, y = am)) +
  geom_hline(yintercept = 0.5, color = "white", size = 2) +
  geom_point() +
  geom_abline(intercept = coef(m91)[1],
              slope = coef(m91)[2],
              color = "blue") 
```




```{r}
pred_mpg_z_0 <- predict(m91, newdata = data.frame(mpg_z = 0))
```


$Pr(\text{am}=1|m91,\text{mpg_z}=0) = 0.46$: 
Die Wahrscheinlichkeit einer manuelle Schaltung, 
gegeben einem durchschnittlichen Verbrauch (und dem Modell `m91`) liegt bei knapp 50%.



]


---

## Lineare Modelle running wild

Wie gro√ü ist die Wahrscheinlichkeit f√ºr eine manuelle Schaltung ...

- ... bei `mpg_z = -2`?

```{r echo = TRUE}
(predict(m91, newdata = data.frame(mpg_z = -2)))
```

$Pr(\hat{y})<0$ macht keinen Sinn. ‚ö° 


- ... bei `mpg_z = +2`?

```{r echo = TRUE}
(predict(m91, newdata = data.frame(mpg_z = 2)))
```


$Pr(\hat{y})>1$ macht keinen Sinn. ‚ö° 



Schauen Sie sich mal die Vorhersage an f√ºr `mpg_z=5` ü§Ø 

---

## Wir m√ºssen die Regressionsgerade umbiegen


.pull-left[
... wenn der vorhergesagte Wert eine Wahrscheinlichkeit, $p_i$, ist.

```{r fig.asp = 1}
s_fun <- function(x) exp(x) / (1 + exp(x))

plot_logist_regr <- 
  ggplot(tibble(x=c(-2,2))) +
  aes(x=x) +
  geom_abline(slope = .25,
              intercept = 0.5) +
  labs(x = "x",
       y = "Pr(y=1)=p") +
  geom_hline(yintercept = c(0,1), linetype = "dotted") +
  scale_x_continuous(limits = c(-5,5)) +
  scale_y_continuous(limits = c(-0.5, 1.5)) +
  stat_function( fun = s_fun, color = "blue", size = 2) +
  geom_rect(xmin = -Inf,
            xmax = +Inf,
            ymin = 1,
            ymax = +Inf,
            fill = "Red",
            alpha = .2) +
  geom_rect(xmin = -Inf,
            xmax = +Inf,
            ymin = 0,
            ymax = -Inf,
            fill = "Red",
            alpha = .2)

plot_logist_regr
```


]

.pull-right[
- Die *schwarze* Gerade verl√§sst den Wertebereich der Wahrscheinlichkeit.

- Die *blaue* Kurve, $\mathcal{f}$, bleibt im erlaubten Bereich, $Pr(y) \in [0,1]$.

- Wir m√ºssen also die linke oder die rechte Seite des linearen Modells transformieren:

$p_i = f(\alpha + \beta \cdot x)$ bzw.:

$f(p) = \alpha + \beta \cdot x$

- $\mathcal{f}$ nennt man eine *Link-Funktion*.



]


`r RefManageR::Citet(bib, "mcelreath_statistical_2020", after = ", Kap. 10.2")`



---

## Verallgemeinerte lineare Modelle zur Rettung

- F√ºr metrische AV mit theoretisch unendlichen Grenzen des Wertebereichs haben wir bisher eine Normalverteilung verwendet:

$$y_i \sim \mathcal{N}(\mu_i, \sigma)$$

- Dann ist die Normalverteilung eine voraussetzungsarme Wahl (maximiert die Entropie).

- Aber wenn die AV *bin√§r* ist bzw. *H√§ufigkeiten* modelliert, braucht man eine Variable die nur positive Werte zul√§sst.

- In diesem Fall passt die *Binomialverteilung*, $\mathcal{Bin}$ oder $\mathcal{B}$, gut und ist voraussetzungsarm (maximiert die Entropie):

$$
\begin{align}
y_i &\sim \mathcal{Bin}(n, p_i) \\
f(p_i) &= \alpha + \beta x
\end{align}
$$
- Eine bin√§re Variablen ist eine H√§ufigkeitsvariable mit $\mathcal{Bin}(1,p)$.

- Diese Verallgemeinerung des linearen Modells bezeichnet man als *generalisiertes lineares Modell* (generalized linear model).

---

## Der Logit-Link

- Der *Logit-Link*, $\mathcal{L}$, `logit`, Log-Odds oder Logit-Funktion genannt, ordnet einen Parameter, der als Wahrscheinlichkeitsmasse definiert ist (und daher im Bereich von 0 bis 1 liegt), einem linearen Modell zu (das jeden beliebigen reellen Wert annehmen kann):

$$
\begin{align}
y_i &\sim \mathcal{B}(n, p_i) \\
\text{logit}(p_i) &= \alpha + \beta x_i
\end{align}
$$

- Die Logit-Funktion $\mathcal{L}$ ist definiert als der (nat√ºrliche) Logarithmus des Verh√§ltnisses der Wahrscheinlichkeit zu Gegenwahrscheinlichkeit:

$$\mathcal{L} = \text{log} \frac{p_i}{1-p_i}$$

- Das Verh√§ltnis der Wahrscheinlichkeit zu Gegenwahrscheinlichkeit nennt man auch *Odds*.

- Also:

$$\mathcal{L} = \text{log} \frac{p_i}{1-p_i} = \alpha + \beta x_i$$


---

## Inverser Logit

Um nach $p$ aufzul√∂sen, m√ºssen wir einige Algebra bem√ºhen:

$$
\begin{align}
\text{log} \frac{p}{1-p} &= \alpha + \beta x & & \text{Exponentieren}\\
\frac{p}{1-p} &= e^{\alpha + \beta x} \\
p_i &= e^{\alpha + \beta x_i} (1-p) & & \text{Zur Vereinfachung: } x := e^{\alpha + \beta x_i} \\
p_i &= x (1-p) \\
&= x - xp \\
p + px &= x \\
p(1+x) &= x \\
p &= \frac{x} {1+x} & & \text{L√∂sen wir x wieder auf.} \\
p &= \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}} = \mathcal{L}^{-1}
\end{align}
$$

Diese Funktion nennt man auch *inverser Logit*, $\text{logit}^{-1}, \mathcal{L}^{-1}$.

---

## Logit und Inverser Logit
.pull-left[
#### Logit
$(0,1) \rightarrow (-\infty, +\infty)$

```{r dpi = 300}
ggplot() +
  annotate("segment", 
           x = 0,
           xend = 0,
           y = 0,
           yend = 1) +
  scale_x_continuous(breaks = c(0,1), labels = c("p", "Logit")) +
  scale_y_continuous(breaks = NULL) +
  annotate("label",
           x = c(0, 0),
             y = c(0, 1),
             label = c("0", "1")) +
  annotate("segment", 
           x = 1,
           xend = 1,
           y = -1,
           yend = 2) +
  annotate("segment", 
           x = 1,
           xend = 1,
           y = -2,
           yend = 3,
           linetype = "dotted") +
  annotate("label",
           x = c(1, 1),
             y = c(-2, 3),
             label = c("-‚àû", "+‚àû")) +
  annotate("segment",
           x = 0.02,
           xend = 0.98,
           y = 0,
           yend = -2,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow()) +
  annotate("segment",
           x = 0.02,
           xend = 0.98,
           y = 1,
           yend = 3,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow()) +
  labs(x = "",
       y = "") +
  annotate("label",
           x = c(1.01),
             y = c(.5),
             label = c("0"),
           size = 4) +
  labs(x = "",
       y = "") +
  annotate("label",
           x = c(-0.01),
             y = c(.5),
             label = c("0.5"),
           size = 4) +
  annotate("segment",
           x = 0.02,
           xend = 0.98,
           yend = .5,
           y = .5,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow())
```

Praktisch, um Wahrscheinlichkeit zu modellieren.

$$p \rightarrow \fbox{logit} \rightarrow \alpha + \beta x$$

]
.pull-right[
#### Inv-Logit
$(-\infty, +\infty) \rightarrow (0,1)$

```{r }
ggplot() +
  annotate("segment", 
           x = 0,
           xend = 0,
           y = 0,
           yend = 1) +
  scale_x_continuous(breaks = c(0,1), labels = c("p", "Logit")) +
  scale_y_continuous(breaks = NULL) +
  annotate("label",
           x = c(0, 0),
             y = c(0, 1),
             label = c("0", "1")) +
  annotate("segment", 
           x = 1,
           xend = 1,
           y = -1,
           yend = 2) +
  annotate("segment", 
           x = 1,
           xend = 1,
           y = -2,
           yend = 3,
           linetype = "dotted") +
  annotate("label",
           x = c(1, 1),
             y = c(-2, 3),
             label = c("-‚àû", "+‚àû")) +
  annotate("segment",
           xend = 0.02,
           x = 0.98,
           yend = 0,
           y = -2,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow()) +
  annotate("segment",
           xend = 0.02,
           x = 0.98,
           yend = 1,
           y = 3,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow()) +
  annotate("label",
           x = c(1.01),
             y = c(.5),
             label = c("0"),
           size = 3) +
  labs(x = "",
       y = "") +
  annotate("label",
           x = c(-0.01),
             y = c(.5),
             label = c("0.5"),
           size = 3) +
  annotate("segment",
           xend = 0.02,
           x = 0.98,
           yend = .5,
           y = .5,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow())
```

Praktisch, um in Wahrscheinlichkeiten umzurechnen.

$$p \leftarrow \fbox{inv-logit} \leftarrow \alpha + \beta x$$

]


---

## Logistische Regression

- Eine Regression mit binomial verteilter AV und Logit-Link nennt man *logistische Regression*.

- Man verwendet die logistische Regression um binomial verteilte AV zu modellieren, z.B.
    - Wie hoch ist die Wahrscheinlichkeit, dass ein Kunde das Produkt kauft?
    - Wie hoch ist die Wahrscheinlichkeit, dass ein Mitarbeiter k√ºndigt?
    - Wie hoch ist die Wahrscheinlichkeit, die Klausur zu bestehen?
    
- Die logistische Regression ist eine normale, lineare Regression f√ºr den Logit von $Pr(y=1)$, wobei $y$ (AV) binomialvereteilt mit $n=1$ angenommen wird:


$$
\begin{align}
y_i &\sim \mathcal{B}(1, p_i) \\
\text{logit}(p_i) &= \alpha + \beta x_i
\end{align}
$$


- Da es sich um eine normale, lineare Regression handelt, sind alle bekannten Methoden und Techniken der linearen Regression zul√§ssig.

- Da Logits nicht einfach zu interpretieren sind, rechnet man nach der Berechnung des Modells den Logit h√§ufig in Wahrscheinlichkeiten um.



---

## Die Koeffizienten sind schwer zu interpretieren

```{r fig.asp=0.4}
s_fun <- function(x) exp(x) / (1 + exp(x))
inv_logist <- function(x) x

p_logist1 <- 
  ggplot(tibble(x=c(-2,2))) +
  aes(x=x) +
  labs(x = "x",
       title = "Logistische Regression",
       y = "Pr(y=1)=p") +
  scale_x_continuous(limits = c(-3,3)) +
  scale_y_continuous(limits = c(-0.5, 1.5)) +
  stat_function( fun = plogis) 

p_logist2 <- 
  ggplot(tibble(x=c(-2,2))) +
  aes(x=x) +
  labs(x = "x",
       title = "Lineare Regression",
       y = "Logit") +
  scale_x_continuous(limits = c(-3,3)) +
  scale_y_continuous(limits = c(-5, 5)) +
  stat_function(fun = inv_logist)  

p_logist1 + p_logist2
```



- In der logistischen Regression gilt *nicht* mehr, dass eine konstante Ver√§nderung in der UV mit einer konstanten Ver√§nderung in der AV einhergeht.
- Stattdessen geht eine konstante Ver√§nderung in der UV mit einer konstanten Ver√§nderung im *Logit* der AV einher.
- Beim logistischen Modell hier gilt, dass in der N√§he von $x=0$ die gr√∂√üte Ver√§nderung in $p$ von statten geht; je weiter weg von $x=0$, desto geringer ist die Ver√§nderung in $p$.

---

## Logits vs. Wahrscheinlichkeiten $p$

.pull-left[
```{r logit-konvert, echo = TRUE, results='hide'}
konvert_logits <-
  tibble(
    logit = c( -10, -3, 
              -2, -1, -0.5, -.25, 
              0, 
              .25, .5, 1, 2, 
              3, 10),
    p = invlogit(logit)
  ) 

```


]

.pull-right[

```{r}
konvert_logits %>% 
  gt() %>% 
  fmt_number(everything(), decimals = 2)
```


]









---



